{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nateraw/huggingpics/blob/main/HuggingPics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApFglcc3TASu"
      },
      "source": [
        "# HuggingPics 🤗🖼️\n",
        "\n",
        "Fine-tune Vision Transformers for **anything** using images found on the web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVyU3ScYZ3Hc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "! pip install transformers pytorch-lightning --quiet\n",
        "! sudo apt -qq install git-lfs\n",
        "! git config --global credential.helper store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ihFempth1zK0"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "from getpass import getpass\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from requests.exceptions import HTTPError\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from huggingface_hub import HfApi, HfFolder, Repository, notebook_login\n",
        "from torch.utils.data import DataLoader\n",
        "from torchmetrics import Accuracy\n",
        "from torchvision.datasets import ImageFolder\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_r5KO0aMIsn"
      },
      "source": [
        "## Init Dataset and Split into Training and Validation Sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U4vCcep26Em6"
      },
      "outputs": [],
      "source": [
        "data_dir = Path('images')\n",
        "ds = ImageFolder(data_dir)\n",
        "indices = torch.randperm(len(ds)).tolist()\n",
        "n_val = math.floor(len(indices) * .15)\n",
        "train_ds = torch.utils.data.Subset(ds, indices[:-n_val])\n",
        "val_ds = torch.utils.data.Subset(ds, indices[-n_val:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABreTsYRoDzy"
      },
      "source": [
        "## Show Some Examples\n",
        "\n",
        "#### ⚠️ Note - The image search API isn't perfect ⚠️\n",
        "\n",
        "You may need to go back, tweak your search terms, and repeat the cells above until the images shown below look good.\n",
        "\n",
        "A few bad images is OK, but if they are completely incorrect, you'll definitely want to try again with different terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbzsRQRe6iY5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "num_examples_per_class = 1\n",
        "i = 1\n",
        "for class_idx, class_name in enumerate(ds.classes):\n",
        "    folder = ds.root / class_name\n",
        "    for image_idx, image_path in enumerate(sorted(folder.glob('*'))):\n",
        "        if image_path.suffix in ds.extensions:\n",
        "            image = Image.open(image_path)\n",
        "            plt.subplot(len(ds.classes), num_examples_per_class, i)\n",
        "            ax = plt.gca()\n",
        "            ax.set_title(\n",
        "                class_name,\n",
        "                size='small',\n",
        "                pad=5,\n",
        "                loc='left',\n",
        "                y=0,\n",
        "                backgroundcolor='white'\n",
        "            )\n",
        "            ax.axis('off')\n",
        "            plt.imshow(image)\n",
        "            i += 1\n",
        "\n",
        "            if image_idx + 1 == num_examples_per_class:\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96LqcgYfLGc8"
      },
      "source": [
        "## Preparing Labels for Our Model's Config\n",
        "\n",
        "By adding `label2id` + `id2label` to our model's config, we'll get friendlier labels in the inference API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mh0yxRKDGDd5"
      },
      "outputs": [],
      "source": [
        "label2id = {}\n",
        "id2label = {}\n",
        "\n",
        "for i, class_name in enumerate(ds.classes):\n",
        "    label2id[class_name] = str(i)\n",
        "    id2label[str(i)] = class_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1aly6Vrx2mM"
      },
      "source": [
        "## Image Classification Collator\n",
        "\n",
        "To apply our transforms to images, we'll use a custom collator class. We'll initialize it using an instance of `ViTFeatureExtractor` and pass the collator instance to `torch.utils.data.DataLoader`'s `collate_fn` kwarg."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "D2n0Mf1u1ymf"
      },
      "outputs": [],
      "source": [
        "class ImageClassificationCollator1:\n",
        "    def __init__(self, feature_extractor):\n",
        "        self.feature_extractor = feature_extractor\n",
        " \n",
        "    def __call__(self, batch):\n",
        "        encodings = self.feature_extractor([x[0] for x in batch], return_tensors='pt')\n",
        "        encodings['labels'] = torch.tensor([x[1] for x in batch], dtype=torch.long)\n",
        "        return encodings "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T62uFtK7LTcz"
      },
      "source": [
        "## Init Feature Extractor, Model, Data Loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YovQELKD4Bu8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    'google/vit-base-patch16-224-in21k',\n",
        "    num_labels=len(label2id),\n",
        "    label2id=label2id,\n",
        "    id2label=id2label\n",
        ")\n",
        "collator = ImageClassificationCollator1(feature_extractor)\n",
        "train_loader = DataLoader(train_ds, batch_size=8, collate_fn=collator, num_workers=2, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=8, collate_fn=collator, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEgf32rC7pQh"
      },
      "source": [
        "# Training\n",
        "\n",
        "⚡ We'll use [PyTorch Lightning](https://pytorchlightning.ai/) to fine-tune our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UIIRpEzW4LFo"
      },
      "outputs": [],
      "source": [
        "class Classifier(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model, lr: float = 2e-5, **kwargs):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters('lr', *list(kwargs))\n",
        "        self.model = model\n",
        "        self.forward = self.model.forward\n",
        "        self.val_acc = Accuracy(\n",
        "            task='multiclass' if model.config.num_labels > 2 else 'binary',\n",
        "            num_classes=model.config.num_labels\n",
        "        )\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        outputs = self(**batch)\n",
        "        self.log(f\"train_loss\", outputs.loss)\n",
        "        return outputs.loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        outputs = self(**batch)\n",
        "        self.log(f\"val_loss\", outputs.loss)\n",
        "        acc = self.val_acc(outputs.logits.argmax(1), batch['labels'])\n",
        "        self.log(f\"val_acc\", acc, prog_bar=True)\n",
        "        return outputs.loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wTWYA1rf3Heg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n",
            "Using 16bit None Automatic Mixed Precision (AMP)\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name    | Type                      | Params\n",
            "------------------------------------------------------\n",
            "0 | model   | ViTForImageClassification | 85.9 M\n",
            "1 | val_acc | MulticlassAccuracy        | 0     \n",
            "------------------------------------------------------\n",
            "85.9 M    Trainable params\n",
            "0         Non-trainable params\n",
            "85.9 M    Total params\n",
            "171.896   Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7b8a6b934ee4456b42a6f1e53227d00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/opt/anaconda3/envs/torch/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "  File \"/opt/anaconda3/envs/torch/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "AttributeError: Can't get attribute 'ImageClassificationCollator1' on <module '__main__' (built-in)>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/opt/anaconda3/envs/torch/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "  File \"/opt/anaconda3/envs/torch/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "AttributeError: Can't get attribute 'ImageClassificationCollator1' on <module '__main__' (built-in)>\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "DataLoader worker (pid(s) 1659, 1660) exited unexpectedly",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1121\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/multiprocessing/queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll(timeout):\n\u001b[0;32m--> 114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
            "\u001b[0;31mEmpty\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m classifier \u001b[39m=\u001b[39m Classifier(model, lr\u001b[39m=\u001b[39m\u001b[39m2e-5\u001b[39m)\n\u001b[1;32m      3\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(accelerator\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m'\u001b[39m, devices\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, precision\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, max_epochs\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(classifier, train_loader, val_loader)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    607\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    609\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    610\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    652\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1103\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1103\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1105\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1182\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1181\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1182\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1195\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1194\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1197\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1267\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1267\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1271\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    151\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 152\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    154\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:121\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[1;32m    120\u001b[0m     batch_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mcurrent\u001b[39m.\u001b[39mready\n\u001b[0;32m--> 121\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(data_fetcher)\n\u001b[1;32m    122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     batch_idx, batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data_fetcher)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:184\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetching_function()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:265\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    263\u001b[0m     \u001b[39m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter)\n\u001b[1;32m    266\u001b[0m         \u001b[39m# consume the batch we just fetched\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:280\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    278\u001b[0m start_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_fetch_start()\n\u001b[1;32m    279\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[1;32m    281\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_profiler()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1283\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1284\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1132\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1133\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[1;32m   1135\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 1659, 1660) exited unexpectedly"
          ]
        }
      ],
      "source": [
        "pl.seed_everything(42)\n",
        "classifier = Classifier(model, lr=2e-5)\n",
        "trainer = pl.Trainer(accelerator='gpu', devices=1, precision=16, max_epochs=4)\n",
        "trainer.fit(classifier, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJLxkmefLZT0"
      },
      "source": [
        "## Check if it Worked 😅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDfUUwH73LSq"
      },
      "outputs": [],
      "source": [
        "val_batch = next(iter(val_loader))\n",
        "outputs = model(**val_batch)\n",
        "print('Preds: ', outputs.logits.softmax(1).argmax(1))\n",
        "print('Labels:', val_batch['labels'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FlEoisI9S-e"
      },
      "source": [
        "# Push to 🤗 Hub 🚀\n",
        "\n",
        "You'll need to authenticate with your Hugging Face account, so make sure to [sign up](https://huggingface.co/join) if you haven't already.\n",
        "\n",
        "Your repo will be created at `{hf_username}/{model_id}`, so pick a `model_id` you like 😊"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1t_A7PkNRiY"
      },
      "outputs": [],
      "source": [
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "t58j9AN-iELM"
      },
      "outputs": [],
      "source": [
        "model_id = \"rare-puppers\" #@param {type:\"string\"}\n",
        "\n",
        "description = \"\"\"\n",
        "Autogenerated by HuggingPics🤗🖼️\n",
        "\n",
        "Create your own image classifier for **anything** by running [the demo on Google Colab](https://colab.research.google.com/github/nateraw/huggingpics/blob/main/HuggingPics.ipynb).\n",
        "\n",
        "Report any issues with the demo at the [github repo](https://github.com/nateraw/huggingpics).\n",
        "\"\"\"\n",
        "task_name = \"Image Classification\"\n",
        "task_type = 'image-classification'\n",
        "metric_name = 'Accuracy'\n",
        "metric_type = 'accuracy'\n",
        "metric_value = trainer.callback_metrics['val_acc'].item()\n",
        "\n",
        "# Delete model folder, as we (re)create it here.\n",
        "if Path('./model').exists():\n",
        "    shutil.rmtree('./model')\n",
        "\n",
        "token = HfFolder().get_token()\n",
        "if not token:\n",
        "    raise RuntimeError(\"You must log in to push to hub! Run notebook_login() in the cell above.\")\n",
        "\n",
        "hf_username = HfApi().whoami()['name']\n",
        "model_url = HfApi().create_repo(token=token, repo_id=model_id, exist_ok=True)\n",
        "model_repo = Repository(\"./model\", clone_from=model_url, use_auth_token=token, git_email=f\"{hf_username}@users.noreply.huggingface.co\", git_user=hf_username)\n",
        "model.save_pretrained(model_repo.local_dir)\n",
        "feature_extractor.save_pretrained(model_repo.local_dir)\n",
        "\n",
        "# Copy over tensorboard logs from lightning_logs/ into ./model/runs/\n",
        "tensorboard_logs_path = next(Path(trainer.logger.log_dir).glob('events.out*'))\n",
        "model_repo_logs_path = Path(model_repo.local_dir) / 'runs'\n",
        "model_repo_logs_path.mkdir(exist_ok=True, parents=True)\n",
        "shutil.copy(tensorboard_logs_path, model_repo_logs_path)\n",
        "\n",
        "# Copy over a few example images\n",
        "example_images_dir = Path(model_repo.local_dir) / 'images'\n",
        "example_images_dir.mkdir(exist_ok=True, parents=True)\n",
        "image_markdown_template = '''\n",
        "#### {class_name}\n",
        "\n",
        "![{class_name}](images/{example_image_path})\n",
        "'''\n",
        "example_images_markdown = \"\"\n",
        "for class_idx, class_name in enumerate(ds.classes):\n",
        "    folder = ds.root / class_name\n",
        "    image_path = sorted(folder.glob('*'))[0]\n",
        "    example_image_path = example_images_dir / f\"{class_name.replace(' ', '_')}{image_path.suffix}\"\n",
        "    shutil.copy(image_path, example_image_path)\n",
        "    example_images_markdown += image_markdown_template.format(\n",
        "        class_name=class_name,\n",
        "        example_image_path=example_image_path.name\n",
        "    )\n",
        "\n",
        "\n",
        "# Prepare README.md from information gathered above\n",
        "readme_txt = f\"\"\"\n",
        "---\n",
        "tags:\n",
        "- image-classification\n",
        "- pytorch\n",
        "- huggingpics\n",
        "metrics:\n",
        "- {metric_type}\n",
        "\n",
        "model-index:\n",
        "- name: {model_id}\n",
        "  results:\n",
        "  - task:\n",
        "      name: {task_name}\n",
        "      type: {task_type}\n",
        "    metrics:\n",
        "      - name: {metric_name}\n",
        "        type: {metric_type}\n",
        "        value: {metric_value}\n",
        "---\n",
        "\n",
        "# {model_id}\n",
        "\n",
        "{description}\n",
        "\n",
        "## Example Images\n",
        "\n",
        "{example_images_markdown}\n",
        "\n",
        "\"\"\".strip()\n",
        "\n",
        "(Path(model_repo.local_dir) / 'README.md').write_text(readme_txt)\n",
        "\n",
        "commit_url = model_repo.push_to_hub()\n",
        "\n",
        "print(\"Check out your model at:\")\n",
        "print(f\"https://huggingface.co/{hf_username}/{model_id}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "d61e67d4406f83661a218a7594034be74564666d0640d3900a3e99845865d0f0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
